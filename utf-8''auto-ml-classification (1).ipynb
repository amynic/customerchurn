{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Automated Machine Learning\n_**Classification with Local Compute**_\n\n## Contents\n1. [Introduction](#Introduction)\n1. [Setup](#Setup)\n1. [Data](#Data)\n1. [Train](#Train)\n1. [Results](#Results)\n1. [Test](#Test)\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Introduction\n\nIn this example we use the scikit-learn's [digit dataset](http://scikit-learn.org/stable/datasets/index.html#optical-recognition-of-handwritten-digits-dataset) to showcase how you can use AutoML for a simple classification problem.\n\nMake sure you have executed the [configuration](../../../configuration.ipynb) before running this notebook.\n\nIn this notebook you will learn how to:\n1. Create an `Experiment` in an existing `Workspace`.\n2. Configure AutoML using `AutoMLConfig`.\n3. Train the model using local compute.\n4. Explore the results.\n5. Test the best fitted model."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Setup\n\nAs part of the setup you have already created an Azure ML `Workspace` object. For AutoML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import logging\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n\nimport azureml.core\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace.from_config()\n\n# Choose a name for the experiment and specify the project folder.\nexperiment_name = 'automl-classification'\nproject_folder = './sample_projects/automl-classification'\n\nexperiment = Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\noutputDf = pd.DataFrame(data = output, index = [''])\noutputDf.T",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data\n\nThis uses scikit-learn's [load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) method."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "digits = datasets.load_digits()\nprint(type(digits))\n\n# Exclude the first 100 rows from training so that they can be used for test.\nX_train = digits.data[100:,:]\ny_train = digits.target[100:]\n\nprint(X_train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd \ndata = pd.read_csv(\"churndataset.csv\") \ndata.head()\nprint(\"Data shape: \" + str(data.shape))\n\n# Remove first 100 rows for testing. X = features so take first 21 features\nX_train = data.iloc[100:,0:20]\n# Remove first 100 rows for testing. y = label so take final churn column\ny_train = data.iloc[100:,20:21].values\ny_train = np.squeeze(y_train)\n\nprint(\"X Shape: \" + str(X_train.shape) + \" and y shape: \" + str(y_train.shape))\nprint(\"X_TRAIN\")\nprint(X_train.head())\nprint(\"Y_TRAIN\")\nprint(y_train.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train\n\nInstantiate an `AutoMLConfig` object to specify the settings and data used to run the experiment.\n\n|Property|Description|\n|-|-|\n|**task**|classification or regression|\n|**primary_metric**|This is the metric that you want to optimize. Classification supports the following primary metrics: <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>average_precision_score_weighted</i><br><i>norm_macro_recall</i><br><i>precision_score_weighted</i>|\n|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n|**iterations**|Number of iterations. In each iteration AutoML trains a specific pipeline with the data.|\n|**n_cross_validations**|Number of cross validation splits.|\n|**X**|(sparse) array-like, shape = [n_samples, n_features]|\n|**y**|(sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]<br>Multi-class targets. An indicator matrix turns on multilabel classification. This should be an array of integers.|\n|**path**|Relative path to the project folder. AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder.|"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "automl_config = AutoMLConfig(task = 'classification',\n                             debug_log = 'automl_errors.log',\n                             primary_metric = 'AUC_weighted',\n                             iteration_timeout_minutes = 60,\n                             iterations = 5,\n                             n_cross_validations = 3,\n                             verbosity = logging.INFO,\n                             X = X_train, \n                             y = y_train,\n                             preprocess=True,\n                             path = project_folder)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Call the `submit` method on the experiment object and pass the run configuration. Execution of local runs is synchronous. Depending on the data and the number of iterations this can run for a while.\nIn this example, we specify `show_output = True` to print currently running iterations to the console."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = experiment.submit(automl_config, show_output = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Optionally, you can continue an interrupted local run by calling `continue_experiment` without the `iterations` parameter, or run more iterations for a completed run by specifying the `iterations` parameter:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = local_run.continue_experiment(X = X_train, \n                                          y = y_train, \n                                          show_output = True,\n                                          iterations = 6)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Results"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Widget for Monitoring Runs\n\nThe widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n\n**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(local_run).show() ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n#### Retrieve All Child Runs\nYou can also use SDK methods to fetch all the child runs and see individual metrics that we log."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "children = list(local_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n    metricslist[int(properties['iteration'])] = metrics\n\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Retrieve the Best Model\n\nBelow we select the best pipeline from our iterations. The `get_output` method returns the best run and the fitted model. The Model includes the pipeline and any pre-processing.  Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = local_run.get_output()\nprint(best_run)\nprint(fitted_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Best Model Based on Any Other Metric\nShow the run and the model that has the smallest `log_loss` value:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lookup_metric = \"log_loss\"\nbest_run, fitted_model = local_run.get_output(metric = lookup_metric)\nprint(best_run)\nprint(fitted_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Model from a Specific Iteration\nShow the run and the model from the third iteration:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "iteration = 3\nthird_run, third_model = local_run.get_output(iteration = iteration)\nprint(third_run)\nprint(third_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test \n\n#### Load Test Data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#digits = datasets.load_digits()\nprint(data.shape)\nX_test = data.iloc[:99,0:20]\ny_test = data.iloc[:99,20:21].values\ny_test = np.squeeze(y_test)\n\nprint(\"X: \" + str(X_test.shape) + \" Y: \" + str(y_test.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Testing Our Best Fitted Model\nWe will try to predict 2 digits and see how our model works."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Randomly select digits and test.\nfor index in np.random.choice(len(y_test), 20, replace = False):\n    #print(index)\n    predicted = fitted_model.predict(X_test[index:index + 1])[0]\n    label = y_test[index]\n    print(\"Index: \" + str(index))\n    print(\" Prediction: \" + str(predicted))\n    print(\" Actual Label: \" + str(label))\n    #title = \"Label value = %d  Predicted value = %d \" % (label, predicted)\n    #fig = plt.figure(1, figsize = (3,3))\n    #ax1 = fig.add_axes((0,0,.8,.8))\n    #ax1.set_title(title)\n    #plt.imshow(images[index], cmap = plt.cm.gray_r, interpolation = 'nearest')\n    #plt.show()",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "savitam"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}