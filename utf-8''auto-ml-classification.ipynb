{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Copyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Automated Machine Learning\n_**Classification with Local Compute**_\n\n## Contents\n1. [Introduction](#Introduction)\n1. [Setup](#Setup)\n1. [Data](#Data)\n1. [Train](#Train)\n1. [Results](#Results)\n1. [Test](#Test)\n\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Introduction\n\nIn this example we use the scikit-learn's [digit dataset](http://scikit-learn.org/stable/datasets/index.html#optical-recognition-of-handwritten-digits-dataset) to showcase how you can use AutoML for a simple classification problem.\n\nMake sure you have executed the [configuration](../../../configuration.ipynb) before running this notebook.\n\nIn this notebook you will learn how to:\n1. Create an `Experiment` in an existing `Workspace`.\n2. Configure AutoML using `AutoMLConfig`.\n3. Train the model using local compute.\n4. Explore the results.\n5. Test the best fitted model."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Setup\n\nAs part of the setup you have already created an Azure ML `Workspace` object. For AutoML you will need to create an `Experiment` object, which is a named object in a `Workspace` used to run experiments."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import logging\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n\nimport azureml.core\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "ws = Workspace.from_config()\n\n# Choose a name for the experiment and specify the project folder.\nexperiment_name = 'automl-classification'\nproject_folder = './sample_projects/automl-classification'\n\nexperiment = Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\noutputDf = pd.DataFrame(data = output, index = [''])\noutputDf.T",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Found the config file in: /home/nbuser/library/how-to-use-azureml/automated-machine-learning/classification/config.json\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Experiment Name</th>\n      <td>automl-classification</td>\n    </tr>\n    <tr>\n      <th>Location</th>\n      <td>northeurope</td>\n    </tr>\n    <tr>\n      <th>Project Directory</th>\n      <td>./sample_projects/automl-classification</td>\n    </tr>\n    <tr>\n      <th>Resource Group</th>\n      <td>customerchurn</td>\n    </tr>\n    <tr>\n      <th>SDK version</th>\n      <td>1.0.17</td>\n    </tr>\n    <tr>\n      <th>Subscription ID</th>\n      <td>a2a1fc9f-5671-4479-8922-ad16e34c0fdc</td>\n    </tr>\n    <tr>\n      <th>Workspace Name</th>\n      <td>customerchurn</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                                          \nExperiment Name    automl-classification                  \nLocation           northeurope                            \nProject Directory  ./sample_projects/automl-classification\nResource Group     customerchurn                          \nSDK version        1.0.17                                 \nSubscription ID    a2a1fc9f-5671-4479-8922-ad16e34c0fdc   \nWorkspace Name     customerchurn                          "
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Data\n\nThis uses scikit-learn's [load_digits](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) method."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "digits = datasets.load_digits()\nprint(type(digits))\n\n# Exclude the first 100 rows from training so that they can be used for test.\nX_train = digits.data[100:,:]\ny_train = digits.target[100:]\n\nprint(X_train.shape)",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "<class 'sklearn.utils.Bunch'>\n(1697, 64)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd \ndata = pd.read_csv(\"churndataset.csv\") \ndata.head()\nprint(\"Data shape: \" + str(data.shape))\n\n# Remove first 100 rows for testing. X = features so take first 21 features\nX_train = data.iloc[100:,0:20]\n# Remove first 100 rows for testing. y = label so take final churn column\ny_train = data.iloc[100:,20:21].values\ny_train = np.squeeze(y_train)\n\nprint(\"X Shape: \" + str(X_train.shape) + \" and y shape: \" + str(y_train.shape))\nprint(\"X_TRAIN\")\nprint(X_train.head())\nprint(\"Y_TRAIN\")\nprint(y_train.shape)\n",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Data shape: (3333, 21)\nX Shape: (3233, 20) and y shape: (3233,)\nX_TRAIN\n    state  account length  area code phone number international plan  \\\n100  IA    98              510        379-6506     no                  \n101  MA    108             415        347-7741     no                  \n102  VT    135             415        354-3783     no                  \n103  KY    95              408        401-7594     no                  \n104  IN    122             408        397-4976     no                  \n\n    voice mail plan  number vmail messages  total day minutes  \\\n100  yes             21                    161.20               \n101  no              0                     178.30               \n102  no              0                     151.70               \n103  no              0                     135.00               \n104  no              0                     170.50               \n\n     total day calls  total day charge  total eve minutes  total eve calls  \\\n100  114             27.40             252.20              83                \n101  137             30.31             189.00              76                \n102  82              25.79             119.00              105               \n103  99              22.95             183.60              106               \n104  94              28.99             173.70              109               \n\n     total eve charge  total night minutes  total night calls  \\\n100 21.44             160.20                92                  \n101 16.07             129.10                102                 \n102 10.12             180.00                100                 \n103 15.61             245.30                102                 \n104 14.76             248.60                75                  \n\n     total night charge  total intl minutes  total intl calls  \\\n100 7.21                4.40                 8                  \n101 5.81                14.60                5                  \n102 8.10                10.50                6                  \n103 11.04               12.50                9                  \n104 11.19               11.30                2                  \n\n     total intl charge  customer service calls  \n100 1.19                4                       \n101 3.94                0                       \n102 2.84                0                       \n103 3.38                1                       \n104 3.05                1                       \nY_TRAIN\n(3233,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Train\n\nInstantiate an `AutoMLConfig` object to specify the settings and data used to run the experiment.\n\n|Property|Description|\n|-|-|\n|**task**|classification or regression|\n|**primary_metric**|This is the metric that you want to optimize. Classification supports the following primary metrics: <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>average_precision_score_weighted</i><br><i>norm_macro_recall</i><br><i>precision_score_weighted</i>|\n|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n|**iterations**|Number of iterations. In each iteration AutoML trains a specific pipeline with the data.|\n|**n_cross_validations**|Number of cross validation splits.|\n|**X**|(sparse) array-like, shape = [n_samples, n_features]|\n|**y**|(sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]<br>Multi-class targets. An indicator matrix turns on multilabel classification. This should be an array of integers.|\n|**path**|Relative path to the project folder. AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder.|"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "automl_config = AutoMLConfig(task = 'classification',\n                             debug_log = 'automl_errors.log',\n                             primary_metric = 'AUC_weighted',\n                             iteration_timeout_minutes = 60,\n                             iterations = 5,\n                             n_cross_validations = 3,\n                             verbosity = logging.INFO,\n                             X = X_train, \n                             y = y_train,\n                             preprocess=True,\n                             path = project_folder)",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Call the `submit` method on the experiment object and pass the run configuration. Execution of local runs is synchronous. Depending on the data and the number of iterations this can run for a while.\nIn this example, we specify `show_output = True` to print currently running iterations to the console."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = experiment.submit(automl_config, show_output = True)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Running on local machine\nParent Run ID: AutoML_672b2e01-41ed-4de0-9dd9-fac506706b54\n********************************************************************************************************************\nITERATION: The iteration being evaluated.\nPIPELINE: A summary description of the pipeline being evaluated.\nSAMPLING %: Percent of the training data to sample.\nDURATION: Time taken for the current iteration.\nMETRIC: The result of computing score on the fitted pipeline.\nBEST: The best observed score thus far.\n********************************************************************************************************************\n\n ITERATION   PIPELINE                                       SAMPLING %  DURATION      METRIC      BEST\n         0   MaxAbsScaler LightGBM                          100.0000    0:00:27       0.8611    0.8611\n         1   MaxAbsScaler LightGBM                          100.0000    0:00:26       0.8798    0.8798\n         2   MaxAbsScaler LightGBM                          100.0000    0:00:25       0.8707    0.8798\n         3   StandardScalerWrapper LightGBM                 100.0000    0:00:24       0.8802    0.8802\n         4   Ensemble                                       100.0000    0:00:35       0.9021    0.9021\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 75,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>automl-classification</td><td>AutoML_28e16be1-8b97-4369-905a-2e9a9e43cc68</td><td>automl</td><td>Completed</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/a2a1fc9f-5671-4479-8922-ad16e34c0fdc/resourceGroups/customerchurn/providers/Microsoft.MachineLearningServices/workspaces/customerchurn/experiments/automl-classification/runs/AutoML_28e16be1-8b97-4369-905a-2e9a9e43cc68\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: automl-classification,\nId: AutoML_28e16be1-8b97-4369-905a-2e9a9e43cc68,\nType: automl,\nStatus: Completed)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Optionally, you can continue an interrupted local run by calling `continue_experiment` without the `iterations` parameter, or run more iterations for a completed run by specifying the `iterations` parameter:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "local_run = local_run.continue_experiment(X = X_train, \n                                          y = y_train, \n                                          show_output = True,\n                                          iterations = 6)",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": "No run_configuration provided, running locally with default configuration\nRunning on local machine\nParent Run ID: AutoML_28e16be1-8b97-4369-905a-2e9a9e43cc68\n********************************************************************************************************************\nITERATION: The iteration being evaluated.\nPIPELINE: A summary description of the pipeline being evaluated.\nSAMPLING %: Percent of the training data to sample.\nDURATION: Time taken for the current iteration.\nMETRIC: The result of computing score on the fitted pipeline.\nBEST: The best observed score thus far.\n********************************************************************************************************************\n\n ITERATION   PIPELINE                                       SAMPLING %  DURATION      METRIC      BEST\n         5   MaxAbsScaler LogisticRegression                100.0000    0:00:30       0.8380    0.9021\n         6   StandardScalerWrapper LightGBM                 100.0000    0:00:25       0.7802    0.9021\n         7   MaxAbsScaler LightGBM                          100.0000    0:00:27       0.8638    0.9021\n         8   MaxAbsScaler LightGBM                          100.0000    0:00:26       0.8881    0.9021\n         9   SparseNormalizer LightGBM                      100.0000    0:00:30       0.9023    0.9023\n        10   Ensemble                                       100.0000    0:00:44       0.9154    0.9154\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Results"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Widget for Monitoring Runs\n\nThe widget will first report a \"loading\" status while running the first iteration. After completing the first iteration, an auto-updating graph and table will be shown. The widget will refresh once per minute, so you should see the graph update as child runs complete.\n\n**Note:** The widget displays a link at the bottom. Use this link to open a web interface to explore the individual run details."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.widgets import RunDetails\nRunDetails(local_run).show() ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6526b4b539124dbea664ca73f2a70a7d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_AutoMLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'sâ€¦"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n#### Retrieve All Child Runs\nYou can also use SDK methods to fetch all the child runs and see individual metrics that we log."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "children = list(local_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n    metricslist[int(properties['iteration'])] = metrics\n\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Retrieve the Best Model\n\nBelow we select the best pipeline from our iterations. The `get_output` method returns the best run and the fitted model. The Model includes the pipeline and any pre-processing.  Overloads on `get_output` allow you to retrieve the best run and fitted model for *any* logged metric or for a particular *iteration*."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "best_run, fitted_model = local_run.get_output()\nprint(best_run)\nprint(fitted_model)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Run(Experiment: automl-classification,\nId: AutoML_672b2e01-41ed-4de0-9dd9-fac506706b54_4,\nType: None,\nStatus: Completed)\nPipeline(memory=None,\n     steps=[('datatransformer', DataTransformer(logger=None, task=None)), ('prefittedsoftvotingclassifier', PreFittedSoftVotingClassifier(classification_labels=None,\n               estimators=[('LightGBM', Pipeline(memory=None,\n     steps=[('standardscalerwrapper', <automl.client.core.common.model_wrappe...er object at 0x7f649f17bdd8>)]))],\n               flatten_transform=None, weights=[0.2, 0.2, 0.6]))])\nY_transformer(['LabelEncoder', LabelEncoder()])\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Best Model Based on Any Other Metric\nShow the run and the model that has the smallest `log_loss` value:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "lookup_metric = \"log_loss\"\nbest_run, fitted_model = local_run.get_output(metric = lookup_metric)\nprint(best_run)\nprint(fitted_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Model from a Specific Iteration\nShow the run and the model from the third iteration:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "iteration = 3\nthird_run, third_model = local_run.get_output(iteration = iteration)\nprint(third_run)\nprint(third_model)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Test \n\n#### Load Test Data"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#digits = datasets.load_digits()\nprint(data.shape)\nX_test = data.iloc[:99,0:20]\ny_test = data.iloc[:99,20:21].values\ny_test = np.squeeze(y_test)\n\nprint(\"X: \" + str(X_test.shape) + \" Y: \" + str(y_test.shape))",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "(3333, 21)\nX: (99, 20) Y: (99,)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "#### Testing Our Best Fitted Model\nWe will try to predict 2 digits and see how our model works."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Randomly select digits and test.\nfor index in np.random.choice(len(y_test), 20, replace = False):\n    #print(index)\n    predicted = fitted_model.predict(X_test[index:index + 1])[0]\n    label = y_test[index]\n    print(\"Index: \" + str(index))\n    print(\" Prediction: \" + str(predicted))\n    print(\" Actual Label: \" + str(label))\n    #title = \"Label value = %d  Predicted value = %d \" % (label, predicted)\n    #fig = plt.figure(1, figsize = (3,3))\n    #ax1 = fig.add_axes((0,0,.8,.8))\n    #ax1.set_title(title)\n    #plt.imshow(images[index], cmap = plt.cm.gray_r, interpolation = 'nearest')\n    #plt.show()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Index: 56\n Prediction: False\n Actual Label: False\nIndex: 18\n Prediction: False\n Actual Label: False\nIndex: 26\n Prediction: False\n Actual Label: False\nIndex: 96\n Prediction: False\n Actual Label: False\nIndex: 71\n Prediction: False\n Actual Label: False\nIndex: 72\n Prediction: False\n Actual Label: False\nIndex: 36\n Prediction: False\n Actual Label: False\nIndex: 10\n Prediction: False\n Actual Label: True\nIndex: 54\n Prediction: False\n Actual Label: True\nIndex: 38\n Prediction: False\n Actual Label: False\nIndex: 11\n Prediction: False\n Actual Label: False\nIndex: 33\n Prediction: True\n Actual Label: True\nIndex: 37\n Prediction: False\n Actual Label: False\nIndex: 2\n Prediction: False\n Actual Label: False\nIndex: 43\n Prediction: False\n Actual Label: False\nIndex: 83\n Prediction: False\n Actual Label: False\nIndex: 16\n Prediction: False\n Actual Label: False\nIndex: 15\n Prediction: True\n Actual Label: True\nIndex: 77\n Prediction: False\n Actual Label: True\nIndex: 44\n Prediction: False\n Actual Label: False\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "savitam"
      }
    ],
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}